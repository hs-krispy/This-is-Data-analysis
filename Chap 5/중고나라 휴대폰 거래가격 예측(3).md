## Feature engineering

EDA를 통해서 얻은 정보를 이용해서 피처 엔지니어링을 진행

- month 피처에서 2016년 10월, 2017년 3월의 데이터가 가장 많은 것을 볼 때 비교적 최근에 가까울수록 중요한 피처
- 휴대폰 동일 기종 내 상대 가격 (z-score)은 정규분포의 형태
-  factory_price는 피처 중요도 분석결과 가장 중요한 피처로 나타났으며 price 피처와 양의 상관 관계를 가짐
- 일부 phone_model이 많은 데이터를 차지 
- maker에서는 apple, samsung, lg가 많은 데이터를 차지하며 피처 중요도 분석결과에서도 maker_apple은 높은 중요도를 가지는 피처로 나타남 

```python
df = pd.read_csv("../data/used_mobile_phone.csv")
from datetime import datetime
import time

def date_to_unixtime(date_str):
    timestamp = time.mktime(datetime.strptime(date_str, '%Y-%m-%d').timetuple())
    return timestamp

df['create_unixtime'] = df['create_date'].apply(lambda x: date_to_unixtime(x[:10]))
# min-max 스케일링 적용
df['create_time_score'] = (df['create_unixtime'] - df['create_unixtime'].min()) / (df['create_unixtime'].max() - df['create_unixtime'].min())
df[['create_date', 'create_unixtime', 'create_time_score']].head()
```

<img src="https://user-images.githubusercontent.com/58063806/100114097-69443200-2eb4-11eb-9a8d-9b45a959e637.JPG" width=100% />

mktime - struct_time 객체를 받아서 시간(초)을 나타내는 float를 반환 (**값이 클수록 최근에 가까워짐**)

<img src="https://user-images.githubusercontent.com/58063806/100114362-b32d1800-2eb4-11eb-8687-b36e76c9a26b.JPG" width=50%/>

create_time_score가 **1에 가까울수록 최근에 작성한** 게시물, **0에 가까울수록 오래된** 게시물을 의미

```python
df['phone_model_storage'] = df['phone_model'].apply(lambda x: x.split(" ")[-1])
df['phone_model_detail'] = df['phone_model'].apply(lambda x: ' '.join(x.split(" ")[:-1]))
df[['phone_model_storage', 'phone_model_detail']].head()
```

phone_model에서 제품명과 용량을 분리

<img src="https://user-images.githubusercontent.com/58063806/100116633-4ff0b500-2eb7-11eb-9a39-56dc68e65166.JPG" width=50% />

```python
model_counts = df['phone_model'].value_counts()
model_detail_counts = df['phone_model_detail'].value_counts()
data = [model_counts, model_detail_counts]
# mpl_fig = plt.figure()
# ax = mpl_fig.add_subplot(111)
# ax.boxplot(data)
plt.boxplot(data)
```

모델명과 용량을 분리하기 전과 후의 분포차이

<img src="https://user-images.githubusercontent.com/58063806/100117255-f3da6080-2eb7-11eb-8c1c-0857e89c6eed.JPG" width=50%/>

모델명과 용량을 분리한 후에는 하나의 기종을 제외하면 전체적인 분포가 조금 더 안정적이 됨

#### 감성 분류

- 텍스트 정보에 대한 피처 엔지니어링 **(가격 예측에 영향을 미칠 물품의 상태가 나타나있는 텍스트 정보)**

- price 피처의 z-score 기준으로 상위5%에 속하는 가격의 게시글은 '상태 좋음', 하위5%는 '상태 나쁨', 나머지는 '보통'으로 판단

```python
df['price_by_group'] = df.groupby('phone_model_detail')['price'].transform(lambda x: (x - x.mean()) / x.std())
ax = df['price_by_group'].hist(bins="auto")
lower_bound = df['price_by_group'].quantile(0.05)
upper_bound = df['price_by_group'].quantile(0.95)
ax.axvline(x=lower_bound, color='r', linestyle='dashed', linewidth=2)
ax.axvline(x=upper_bound, color='r', linestyle='dashed', linewidth=2)
print(lower_bound)
print(upper_bound)
```

<img src="https://user-images.githubusercontent.com/58063806/100234898-0f547280-2f6f-11eb-8941-b3709da5a85b.JPG" width=50%/>

```python
def get_price_level(price, lower, upper):
    if price <= lower:
        return "0"
    elif price >= upper:
        return "2"
    else:
        return "1"
    
df['price_lower'] = df.groupby('phone_model_detail')['price'].transform(lambda x: x.quantile(0.05))
df['price_upper'] = df.groupby('phone_model_detail')['price'].transform(lambda x: x.quantile(0.95))
df['price_level'] = df.apply(lambda row: get_price_level(row['price'], row['price_lower'], row['price_upper']), axis=1)
df[['price', 'price_lower', 'price_upper', 'price_level', 'text']].head()
```

lower_bound이하면 0, upper_bound이상이면 2, 중간이면 1로 가격의 상태를 분류

<img src="https://user-images.githubusercontent.com/58063806/100235921-57c06000-2f70-11eb-8342-9824ee752cb8.JPG" width=90% />

### 텍스트 전처리

```python
import pickle
import re
from konlpy.tag import Okt

# 중고나라 불용어 사전을 로드
with open('../data/used_mobile_phone_stopwords.pkl', 'rb') as f:
    stopwords = pickle.load(f)
    
print(stopwords[:10])
# ['거래', '입니', '판매', '아이폰', '갤럭시', '골드', '팝', '만원', '폰', '시']

def text_cleaning(text):
    # isalnum - 글자 또는 숫자로 구성되어있으면 True, 아니면 False
    # isdigit - 문자열이 숫자면 True, 아니면 False
    text = ''.join(c for c in text if c.isalnum() or c in '+, ')
    text = ''.join([i for i in text if not i.isdigit()])
    return text

def get_pos(x):
    tagger = Okt()
    poses = tagger.pos(x)
    return [pos[0] for pos in poses if pos[0] not in stopwords]

df['text'] = df['text'].apply(lambda x: text_cleaning(x))
result = get_pos(df['text'][0])
print(result)
```

**불용어 - 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요, 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어** 

**EX) I, my, me, over, 조사, 접미사 같은 단어들**

text_cleaning - '+'를 제외한 특수문자를 제거,  숫자형태의 문자를 제거

get_pos - 불용어에 등장하지 않는 형태소만을 추출해서 반환

<img src="https://user-images.githubusercontent.com/58063806/100543143-c4e63500-3291-11eb-9d90-63635d5ac4e3.JPG" width=100% />

```python
from collections import Counter

corpus = sum(df['text'].apply(lambda x: get_pos(x)).tolist(), [])

counter = Counter(corpus) # 요소 개수를 구해서 Dictionary 형태로 반환
# most_common - 요소들 중 빈도수가 높은 상위 n개를 list안의 tuple 형태로 반환
common_words = [key for key, _ in counter.most_common(2500)]
common_words
```

추출한 형태소에서 가장 빈도수가 높은 2500개의 형태소를 선정

<img src="https://user-images.githubusercontent.com/58063806/100543147-c6176200-3291-11eb-8a50-89b5cd48d8ef.JPG" width=13% />