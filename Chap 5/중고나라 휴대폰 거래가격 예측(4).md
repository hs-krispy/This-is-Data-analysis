### TF-IDF 피처 변환

```python
from konlpy.tag import Okt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

def get_common_pos(x):
    tagger = Okt()
    poses = tagger.pos(x)
    return [pos[0] for pos in poses if pos[0] in common_words]

# 1:3:1 비율
negative_random = df[df['price_level']=='0'].sample(321, random_state=30)
neutral_random = df[df['price_level']=='1'].sample(321 * 3, random_state=30)
positive_random = df[df['price_level']=='2'].sample(321, random_state=30)

df_sample = negative_random.append(neutral_random).append(positive_random)

index_vectorizer = CountVectorizer(tokenizer=lambda x: get_common_pos(x))
X = index_vectorizer.fit_transform(df_sample['text'].tolist())
tfidf_vectorizer = TfidfTransformer()
X = tfidf_vectorizer.fit_transform(X)

y = df_sample['price_level']
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)
print(x_train.shape)
print(x_test.shape)
# (1284, 2476)
# (321, 2476)
```

클래스의 불균형을 맞춰주기 위해 부정:중립:긍정 데이터를 1:3:1의 비율로 추출

train 데이터는 1284개, test 데이터는 321개 형태소를 표현하는 피처는 2476개가 생성

#### 분류 모델 학습(SVM)

```python
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.svm import SVC

svm = SVC(kernel='rbf', C=10.0, random_state=0, gamma=0.1)
svm.fit(x_train, y_train)
y_pred_ksvc = svm.predict(x_test)
print("Accuracy: %.2f" % accuracy_score(y_test, y_pred_ksvc))

confmat = confusion_matrix(y_test, y_pred_ksvc)
print(confmat)
```

<img src="https://user-images.githubusercontent.com/58063806/100750313-31476c80-3429-11eb-80a1-16fdea58114b.JPG" width=20% />

### Random forest regressor 재학습

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

X = index_vectorizer.transform(df['text'].tolist())
X = tfidf_vectorizer.transform(X)
df['product_status'] = pd.Series(svm.predict(X))
df = df[['price', 'factory_price', 'maker', 'price_index', 'create_time_score', 'phone_model_storage', 
         'phone_model_detail', 'product_status']]
df = pd.get_dummies(df, columns=['maker', 'phone_model_storage', 'phone_model_detail', 'product_status'])

X = df.loc[:, df.columns != 'price']
y = df['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

forest = RandomForestRegressor(n_estimators=1000, criterion='mse')
forest.fit(X_train, y_train)
y_train_pred = forest.predict(X_train)
y_test_pred = forest.predict(X_test)
print('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)))
print('R2 train: %.3f, test: %.3f' % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))
# MSE train: 4778460514.289, test: 11037517874.698
# R2 train: 0.901, test: 0.748  

importances = forest.feature_importances_
plt.plot(importances, "o")

feat_labels = X.columns.tolist()
feature = list(zip(feat_labels, forest.feature_importances_))
sorted(feature, key=lambda tup: tup[1], reverse=True)[:10]
```

Train R2, Test R2 score 등의 지표가 매우 상승 (하지만 여전히 과적합 발생)

<img src="https://user-images.githubusercontent.com/58063806/100750602-9bf8a800-3429-11eb-9183-2df73ab3f282.JPG" width=50% />

create_time_score, product_status와 같이 새로운 피처의 중요도가 상위에 있는 것을 볼 수 있음

```python
plt.scatter(y_test.values, y_test_pred)
plt.ylabel("predict price")
plt.xlabel("price")
```

<img src="https://user-images.githubusercontent.com/58063806/100751283-75873c80-342a-11eb-849f-525fc5d7590c.JPG" width=50% />

y축은 예측 가격, x축은 실제 가격으로 몇몇 데이터를 제외하고는 대부분 예측이 잘 수행된 것을 볼 수 있음