## 피처 엔지니어링

```python
df_train = pd.read_csv("../data/titanic_train.csv")
df_test = pd.read_csv("../data/titanic_test.csv")
df_train = df_train.drop(['ticket', 'body', 'home.dest'], axis=1)
df_test = df_test.drop(['ticket', 'body', 'home.dest'], axis=1)

replace_mean = df_train[df_train['age'] > 0]['age'].mean()
replace_mean = df_train[df_train['age'] > 0]['age'].mean()
df_train['age'] = df_train['age'].fillna(replace_mean)
df_test['age'] = df_test['age'].fillna(replace_mean)

embarked_mode = df_train['embarked'].value_counts().index[0]
df_train['embarked'] = df_train['embarked'].fillna(embarked_mode)
df_test['embarked'] = df_test['embarked'].fillna(embarked_mode)


whole_df = df_train.append(df_test)
train_idx_num = len(df_train)

print(whole_df['cabin'].value_counts()[:10])
```

cabin 피처는 선실의 정보를 나타내는 데이터 (선실을 대표하는 알파벳이 반드시 첫 글자에 등장)

<img src="https://user-images.githubusercontent.com/58063806/99877833-46abe200-2c44-11eb-9900-24ca231e82dc.JPG" width=20% />

```python
whole_df['cabin'] = whole_df['cabin'].fillna('X')
whole_df['cabin'] = whole_df['cabin'].apply(lambda x: x[0])
print(whole_df['cabin'].value_counts())
whole_df['cabin'] = whole_df['cabin'].replace({"G":"X", "T":"X"})
ax = sns.countplot(x='cabin', hue='survived', data=whole_df)
plt.show()
```

- 결측값은 알파벳 X로 대체하고 cabin 피처의 내용을 알파벳으로 변환

- 수가 너무 적은 G와 T 역시 결측값을 채우는 X로 대체

<img src="https://user-images.githubusercontent.com/58063806/99877921-e10c2580-2c44-11eb-9bc3-7a4ced66f828.JPG" width=10% />

<img src="https://user-images.githubusercontent.com/58063806/99877834-47447880-2c44-11eb-80eb-74dceccef363.JPG" width=50% />

생존자/비생존자의 분포를 보면 유의미한 차이가 있음 

```python
print(whole_df['name'])
name_grade = whole_df['name'].apply(lambda x: x.split(", ", 1)[1].split(".")[0])
name_grade = name_grade.unique().tolist()
print(name_grade)
```

- name 피처의 내용은 성, 호칭, 이름으로 구성되는 공통점이 있음
- 사회적 계급이 존재했던 당시의 시대를 고려

<img src="https://user-images.githubusercontent.com/58063806/99878055-064d6380-2c46-11eb-8036-e3ee003257cd.JPG" width=100% />

```python
grade_dict = {'A': ['Rev', 'Col', 'Major', 'Dr', 'Capt', 'Sir'],  # 명예직
              'B': ['Ms', 'Mme', 'Mrs', 'Dona'], # 여성
              'C': ['Jonkheer', 'the Countess'], # 귀족이나 작위
              'D': ['Mr', 'Don'], # 남성
              'E': ['Master'], # 젊은 남성
              'F': ['Miss', 'Mlle', 'Lady']} # 젊은 여성

def give_grade(x):
    grade = x.split(", ", 1)[1].split(".")[0]
    for key, value in grade_dict.items():
        for title in value:
            if grade == title:
                return key
    return 'G'

whole_df['name'] = whole_df['name'].apply(lambda x: give_grade(x))
print(whole_df['name'].value_counts())
ax = sns.countplot(x='name', hue='survived', data=whole_df)
plt.show()
```

호칭을 이용해서 name 피처의 내용을 A ~ F의 범주형 데이터로 변환

<img src="https://user-images.githubusercontent.com/58063806/99878425-92608a80-2c48-11eb-8feb-722d41abbdeb.JPG" width=13% />

<img src="https://user-images.githubusercontent.com/58063806/99878478-faaf6c00-2c48-11eb-8d6b-40d61cafc3dc.JPG" width=50% />

cabin 피처와 마찬가지로 생존자/비생존자의 분포를 보면 유의미한 차이가 있음 

#### 원 핫 인코딩

```python
whole_df_encoded = pd.get_dummies(whole_df)
df_train = whole_df_encoded[:train_idx_num]
df_test = whole_df_encoded[train_idx_num:]
df_train.head()
```

<img src="https://user-images.githubusercontent.com/58063806/99878554-98a33680-2c49-11eb-901b-e9ab331c8860.JPG" width=100% />

#### 피처 엔지니어링 후 학습

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
x_train, y_train = df_train.loc[:, df_train.columns != 'survived'].values, df_train['survived'].values
x_test, y_test = df_test.loc[:, df_test.columns != 'survived'].values, df_test['survived'].values
lr = LogisticRegression(random_state=0)
lr.fit(x_train, y_train)
y_pred = lr.predict(x_test)
y_pred_probability = lr.predict_proba(x_test)[:, 1]
print("Accuracy: %.2f" % accuracy_score(y_test, y_pred))
print("Precision: %.3f" % precision_score(y_test, y_pred))
print("Recall: %.3f" % recall_score(y_test, y_pred))
print("F1: %.3f" % f1_score(y_test, y_pred))
# Accuracy: 0.79
# Precision: 0.736
# Recall: 0.701
# F1: 0.718

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_probability)
roc_auc = roc_auc_score(y_test, y_pred_probability)
print("AUC l %.3f" % roc_auc)
# AUC l 0.853

plt.rcParams['figure.figsize'] = [5, 4]
plt.plot(false_positive_rate, true_positive_rate, label="ROC curve (area = %0.3f)" % roc_auc, color="r", linewidth=4.0)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve of Logistic regression')
plt.legend(loc="lower right")
```

<img src="https://user-images.githubusercontent.com/58063806/99878612-1f581380-2c4a-11eb-8e5a-dc4e4cb08c76.JPG" width=50% />

#### 피처들의 영향력

```python
cols = df_train.columns.tolist()
cols.remove('survived')
y_pos = np.arange(len(cols))
plt.rcParams['figure.figsize'] = [7, 6]
fig, ax = plt.subplots()
ax.barh(y_pos, lr.coef_[0], align='center', color='g')
ax.set_yticks(y_pos)
ax.set_yticklabels(cols)
ax.invert_yaxis() # y축 순서를 반대로
ax.set_xlabel('Coef')
ax.set_title('Each Feature Coef')

plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/99878861-afe32380-2c4b-11eb-9a77-e9c31f6d6e98.JPG" width=55% />

name, cabin 피처의 영향력이 큰 것을 볼 수 있음